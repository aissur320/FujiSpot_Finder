# flickr_scraper.py (å®‰å…¨ç‰ˆ)

import flickrapi
import requests
import os
import time
import random

# ================= æœ€ç»ˆé…ç½®åŒºåŸŸ =================
# ğŸ”´ è¯·å†æ¬¡ç¡®è®¤ Key æ˜¯å¦æ­£ç¡®
API_KEY = '**********************'
API_SECRET = '****************'

BACKEND_URL = 'http://127.0.0.1:5000/detect'
TEMP_DIR = './temp_flickr_images'
os.makedirs(TEMP_DIR, exist_ok=True)
# ===========================================

def get_flickr_photos(keyword, max_count=50):
    print(f"\nğŸ“¡ è¿æ¥ Flickr æœç´¢: '{keyword}' (ç›®æ ‡: {max_count}å¼ )...")
    try:
        flickr = flickrapi.FlickrAPI(API_KEY, API_SECRET, format='parsed-json')
        photos = flickr.photos.search(
            text=keyword,
            has_geo=1,
            accuracy=11, 
            content_type=1,
            media='photos',
            per_page=max_count,
            # è¯·æ±‚åŸå›¾(url_o)å’Œä¸­å¤§å›¾ï¼Œä¿è¯ç”»è´¨
            extras='geo,url_m,url_l,url_o',
            sort='relevance' # æˆ–è€… 'interestingness-desc'
        )
        return photos['photos']['photo']
    except Exception as e:
        print(f"âŒ APIè¿æ¥å¤±è´¥: {e}")
        return []

def process_photo(photo):
    """
    å¤„ç†å•å¼ ç…§ç‰‡
    è¿”å›: True (è¿›è¡Œäº†ä¸‹è½½ï¼Œéœ€è¦ä¼‘æ¯), False (ä½¿ç”¨äº†ç¼“å­˜ï¼Œä¸éœ€è¦ä¼‘æ¯)
    """
    # å°è¯•è·å–ä»»æ„å¯ç”¨çš„ URL
    url = photo.get('url_m') or photo.get('url_l') or photo.get('url_o')
    if not url: return False

    lat = photo['latitude']
    lon = photo['longitude']
    title = photo['title'] if photo['title'] else f"fuji_{photo['id']}"
    
    # æ¸…ç†æ–‡ä»¶å
    safe_title = "".join([c for c in title if c.isalnum() or c in (' ','-','_')]).strip()
    if len(safe_title) > 30: safe_title = safe_title[:30]
    filename = f"{safe_title}_{photo['id']}.jpg"
    save_path = os.path.join(TEMP_DIR, filename)

    is_new_download = False

    # --- 1. æ£€æŸ¥æœ¬åœ°ç¼“å­˜ ---
    if os.path.exists(save_path):
        # âœ… å‘½ä¸­ç¼“å­˜
        print(f"   ğŸš€ [ç¼“å­˜å‘½ä¸­] {filename[:15]}... -> å‘é€åç«¯")
    else:
        # ğŸ“¥ ä¸‹è½½æ–°å›¾
        print(f"   ğŸ“¥ [ä¸‹è½½æ–°å›¾] {filename[:15]}... (Lat:{lat}, Lon:{lon})")
        try:
            img_data = requests.get(url, timeout=10).content
            with open(save_path, 'wb') as f:
                f.write(img_data)
            is_new_download = True
        except Exception as e:
            print(f"     âŒ ä¸‹è½½å¤±è´¥: {e}")
            return False

    # --- 2. å‘é€ç»™åç«¯ ---
    try:
        with open(save_path, 'rb') as f:
            files = {'image': f}
            data = {'lat': lat, 'lon': lon}
            resp = requests.post(BACKEND_URL, files=files, data=data)
            
        if resp.status_code != 200:
            print(f"     âŒ åç«¯æŠ¥é”™: {resp.status_code}")

    except Exception as e:
        print(f"     âŒ åç«¯è¿æ¥å¤±è´¥: {e}")

    return is_new_download

def main():
    # å…¨æ–¹ä½è¦†ç›–å…³é”®è¯
    keywords = [
        # --- åŒ—ä¾§ (ç»å…¸æ¹–æ™¯) ---
        "Lake Kawaguchiko Mt Fuji",   # æ²³å£æ¹– (é‡å¤§)
        "Lake Yamanaka Mt Fuji",      # å±±ä¸­æ¹– (ä¸œä¾§å¤§æ¹–ï¼Œå¯ä»¥çœ‹åˆ°å¾ˆå¤§çš„å¯Œå£«å±±)
        "Lake Motosu Mt Fuji",        # æœ¬æ –æ¹– (1000æ—¥å…ƒçº¸å¸èƒŒåçš„é£æ™¯)
        "Lake Shoji Mt Fuji",         # ç²¾è¿›æ¹– (è¾ƒå°ï¼Œä½†å®‰é™)
        "Lake Saiko Mt Fuji",         # è¥¿æ¹–
        
        # --- è‘—ååœ°æ ‡ ---
        "Chureito Pagoda",            # æ–°ä»“å±±æµ…é—´å…¬å›­ (å¿ çµå¡” - å¿…æ€æœºä½ï¼)
        "Oshino Hakkai",              # å¿é‡å…«æµ· (è‘—åçš„æ¶Œæ³‰ç¾¤)
        "Lake Tanuki Mt Fuji",        # ç”°è´¯æ¹– (è¥¿ä¾§ï¼Œéå¸¸æœ‰åçš„â€œé’»çŸ³å¯Œå£«â€å€’å½±ç‚¹)
        "Shiraito Falls",             # ç™½ä¸ç€‘å¸ƒ (ç€‘å¸ƒ+å¯Œå£«å±±)
        
        # --- åŸå¸‚è§†è§’ ---
        "Fujinomiya Mt Fuji",         # å¯Œå£«å®«å¸‚ (å—ä¾§ï¼Œå·¨å¤§è€Œéœ‡æ’¼çš„å±±ä½“)
        "Gotemba Mt Fuji",            # å¾¡æ®¿åœº (ä¸œå—ä¾§ï¼Œå¥¥ç‰¹è±æ–¯é™„è¿‘)
        "Mishima Skywalk"             # ä¸‰å²› (è¿œçœº)
    ]
    
    # ğŸ”¥ æœ€ç»ˆè®¾å®š: æ¯ä¸ªè¯æŠ“ 50 å¼ 
    target_per_keyword = 50 
    
    print(f"ğŸš€ å¼€å§‹æœ€ç»ˆçˆ¬å–ä»»åŠ¡: {len(keywords)} ä¸ªå…³é”®è¯ x {target_per_keyword} å¼ /è¯")
    print("----------------------------------------------------")

    for kw in keywords:
        photos = get_flickr_photos(kw, max_count=target_per_keyword)
        print(f"ğŸ” å…³é”®è¯ '{kw}' æ‰¾åˆ° {len(photos)} å¼ å›¾ç‰‡...")
        
        for photo in photos:
            did_download = process_photo(photo)
            
            if did_download:
                # æ–°å›¾ä¸‹è½½åä¼‘æ¯ï¼Œé˜²æ­¢å°å·
                sleep_time = random.uniform(1.5, 3.0)
                print(f"     ğŸ’¤ ä¼‘æ¯ {sleep_time:.1f} ç§’...")
                time.sleep(sleep_time)
            else:
                # ç¼“å­˜å‘½ä¸­ï¼Œæé€Ÿå¤„ç†
                pass

    print("\nğŸ‰ğŸ‰ğŸ‰ æœ€ç»ˆæ•°æ®å¯¼å…¥å®Œæˆï¼è¯·å»å‰ç«¯è§è¯å¥‡è¿¹æ—¶åˆ»ï¼ ğŸ‰ğŸ‰ğŸ‰")

if __name__ == '__main__':
    main()