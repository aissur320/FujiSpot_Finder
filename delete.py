import hashlib
import os

def remove_duplicates(directory):
    # å­˜å‚¨å›¾ç‰‡hashå€¼çš„å­—å…¸
    hashes = {}
    duplicates = []
    
    print(f"æ­£åœ¨æ‰«æé‡å¤å›¾ç‰‡: {directory} ...")
    
    file_list = sorted(os.listdir(directory)) # æ’åºä¿è¯åˆ é™¤é¡ºåºä¸€è‡´
    
    for filename in file_list:
        if not filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):
            continue
            
        filepath = os.path.join(directory, filename)
        
        try:
            # è®¡ç®—æ–‡ä»¶çš„MD5å“ˆå¸Œ
            with open(filepath, 'rb') as f:
                filehash = hashlib.md5(f.read()).hexdigest()
            
            if filehash in hashes:
                duplicates.append(filepath)
                print(f"ğŸ—‘ï¸ å‘ç°é‡å¤: {filename} (ä¸ {hashes[filehash]} é‡å¤)")
                os.remove(filepath) # è‡ªåŠ¨åˆ é™¤
            else:
                hashes[filehash] = filename
        except Exception as e:
            print(f"Error processing {filename}: {e}")

    print(f"âœ… æ¸…ç†å®Œæˆ! å…±åˆ é™¤äº† {len(duplicates)} å¼ é‡å¤å›¾ç‰‡ã€‚\n")

if __name__ == "__main__":
    # è¿™é‡Œå¡«ä½ ä¸‹è½½å›¾ç‰‡çš„è·¯å¾„
    remove_duplicates("./raw_datasets/positive")
    remove_duplicates("./raw_datasets/negative")